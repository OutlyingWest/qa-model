{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Evaluation: MCQ & SAQ\n",
    "\n",
    "This notebook allows you to evaluate model predictions against ground truth for:\n",
    "- **MCQ** (Multiple Choice Questions)\n",
    "- **SAQ** (Short Answer Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcq-header",
   "metadata": {},
   "source": [
    "## MCQ Evaluation\n",
    "\n",
    "Set the paths below and run the cell to evaluate MCQ predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mcq-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCQ_GROUND_TRUTH = \"/home/h5/albu670g/qa-model/results/codex/mcq_gpt-5.2_submission.tsv\"\n",
    "MCQ_PREDICTION = \"/home/h5/albu670g/qa-model/outputs/2026-01-17_00-58-43/mcq_submission.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mcq-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MCQ Results ===\n",
      "Ground Truth: /home/h5/albu670g/qa-model/results/codex/mcq_gpt-5.2_submission.tsv\n",
      "Prediction:   /home/h5/albu670g/qa-model/outputs/2026-01-17_00-58-43/mcq_submission.tsv\n",
      "\n",
      "Total questions: 419\n",
      "Correct:         318\n",
      "Missing/Invalid: 0\n",
      "Accuracy:        0.7589 (75.89%)\n"
     ]
    }
   ],
   "source": [
    "def _pick_mcq_choice(row):\n",
    "    \"\"\"Extract the chosen answer (A/B/C/D) from True/False columns.\"\"\"\n",
    "    true_cols = [c for c in [\"A\", \"B\", \"C\", \"D\"] if str(row[c]).strip().lower() == \"true\"]\n",
    "    if len(true_cols) != 1:\n",
    "        return None\n",
    "    return true_cols[0]\n",
    "\n",
    "def load_mcq_predictions(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load MCQ submission file and extract predicted choices.\"\"\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df[\"choice\"] = df.apply(_pick_mcq_choice, axis=1)\n",
    "    dupes = df[\"MCQID\"].duplicated().sum()\n",
    "    df = df[[\"MCQID\", \"choice\"]].drop_duplicates(subset=\"MCQID\", keep=\"first\")\n",
    "    return df, dupes\n",
    "\n",
    "# Load ground truth and predictions\n",
    "mcq_gt, mcq_gt_dupes = load_mcq_predictions(MCQ_GROUND_TRUTH)\n",
    "mcq_gt = mcq_gt.rename(columns={\"choice\": \"gt_choice\"})\n",
    "mcq_pred, mcq_pred_dupes = load_mcq_predictions(MCQ_PREDICTION)\n",
    "mcq_pred = mcq_pred.rename(columns={\"choice\": \"pred_choice\"})\n",
    "\n",
    "if mcq_gt_dupes > 0 or mcq_pred_dupes > 0:\n",
    "    print(f\"Warning: Removed duplicates - GT: {mcq_gt_dupes}, Pred: {mcq_pred_dupes}\")\n",
    "    print()\n",
    "\n",
    "# Merge and calculate accuracy\n",
    "mcq_merged = mcq_gt.merge(mcq_pred, on=\"MCQID\", how=\"left\")\n",
    "mcq_correct = (mcq_merged[\"pred_choice\"] == mcq_merged[\"gt_choice\"]).sum()\n",
    "mcq_total = len(mcq_merged)\n",
    "mcq_missing = mcq_merged[\"pred_choice\"].isna().sum()\n",
    "mcq_accuracy = mcq_correct / mcq_total\n",
    "\n",
    "print(f\"=== MCQ Results ===\")\n",
    "print(f\"Ground Truth: {MCQ_GROUND_TRUTH}\")\n",
    "print(f\"Prediction:   {MCQ_PREDICTION}\")\n",
    "print(f\"\")\n",
    "print(f\"Total questions: {mcq_total}\")\n",
    "print(f\"Correct:         {mcq_correct}\")\n",
    "print(f\"Missing/Invalid: {mcq_missing}\")\n",
    "print(f\"Accuracy:        {mcq_accuracy:.4f} ({mcq_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saq-header",
   "metadata": {},
   "source": [
    "## SAQ Evaluation\n",
    "\n",
    "Set the paths below and run the cell to evaluate SAQ predictions.\n",
    "\n",
    "The ground truth file should be a TSV with columns: `ID`, `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "saq-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAQ PATHS (edit these) ===\n",
    "SAQ_GROUND_TRUTH = \"/home/h5/albu670g/qa-model/results/codex/saq_gpt-5.2_prediction.tsv\"\n",
    "SAQ_PREDICTION = \"/home/h5/albu670g/qa-model/outputs/__refined_prompt_24_tokens/saq_prediction.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "saq-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAQ Results ===\n",
      "Ground Truth: /home/h5/albu670g/qa-model/results/codex/saq_gpt-5.2_prediction.tsv\n",
      "Prediction:   /home/h5/albu670g/qa-model/outputs/__refined_prompt_24_tokens/saq_prediction.tsv\n",
      "Alignment:    row-order\n",
      "Duplicate IDs (kept): GT=270, Pred=270\n",
      "\n",
      "Total questions: 667\n",
      "Correct:         137\n",
      "Missing/Invalid: 0\n",
      "Accuracy:        0.2054 (20.54%)\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth + predictions (keep all rows; IDs may repeat)\n",
    "saq_gt_raw = pd.read_csv(SAQ_GROUND_TRUTH, sep=\"\\t\", dtype=str)\n",
    "saq_pred_raw = pd.read_csv(SAQ_PREDICTION, sep=\"\\t\", dtype=str)\n",
    "\n",
    "def _norm_answer(x: object) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(x).lower().strip())\n",
    "\n",
    "for name, df in [(\"GT\", saq_gt_raw), (\"Pred\", saq_pred_raw)]:\n",
    "    missing = {\"ID\", \"answer\"} - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} is missing columns: {sorted(missing)}\")\n",
    "\n",
    "saq_gt = saq_gt_raw[[\"ID\", \"answer\"]].copy()\n",
    "saq_pred = saq_pred_raw[[\"ID\", \"answer\"]].copy()\n",
    "\n",
    "saq_gt_dupes = int(saq_gt[\"ID\"].duplicated().sum())\n",
    "saq_pred_dupes = int(saq_pred[\"ID\"].duplicated().sum())\n",
    "\n",
    "alignment_mode = None\n",
    "if len(saq_gt) == len(saq_pred) and saq_gt[\"ID\"].tolist() == saq_pred[\"ID\"].tolist():\n",
    "    alignment_mode = \"row-order\"\n",
    "    saq_merged = pd.DataFrame({\n",
    "        \"ID\": saq_gt[\"ID\"],\n",
    "        \"gt_answer\": saq_gt[\"answer\"],\n",
    "        \"pred_answer\": saq_pred[\"answer\"],\n",
    "    })\n",
    "else:\n",
    "    alignment_mode = \"id+occurrence\"\n",
    "    print(\"Warning: SAQ ID order/length differs; aligning by (ID, occurrence).\")\n",
    "    saq_gt = saq_gt.reset_index(drop=True)\n",
    "    saq_pred = saq_pred.reset_index(drop=True)\n",
    "    saq_gt[\"_occ\"] = saq_gt.groupby(\"ID\").cumcount()\n",
    "    saq_pred[\"_occ\"] = saq_pred.groupby(\"ID\").cumcount()\n",
    "    saq_merged = saq_gt.merge(saq_pred, on=[\"ID\", \"_occ\"], how=\"left\", suffixes=(\"_gt\", \"_pred\"))\n",
    "    saq_merged = saq_merged.rename(columns={\"answer_gt\": \"gt_answer\", \"answer_pred\": \"pred_answer\"})\n",
    "\n",
    "saq_merged[\"gt_norm\"] = saq_merged[\"gt_answer\"].map(_norm_answer)\n",
    "saq_merged[\"pred_norm\"] = saq_merged[\"pred_answer\"].map(_norm_answer)\n",
    "saq_merged[\"is_correct\"] = (\n",
    "    (saq_merged[\"gt_norm\"] != \"\")\n",
    "    & (saq_merged[\"pred_norm\"] != \"\")\n",
    "    & (saq_merged[\"gt_norm\"] == saq_merged[\"pred_norm\"])\n",
    ")\n",
    "\n",
    "saq_total = len(saq_merged)\n",
    "saq_correct = int(saq_merged[\"is_correct\"].sum())\n",
    "saq_missing = int((saq_merged[\"pred_norm\"] == \"\").sum())\n",
    "saq_accuracy = saq_correct / saq_total if saq_total > 0 else 0\n",
    "\n",
    "print(f\"=== SAQ Results ===\")\n",
    "print(f\"Ground Truth: {SAQ_GROUND_TRUTH}\")\n",
    "print(f\"Prediction:   {SAQ_PREDICTION}\")\n",
    "print(f\"Alignment:    {alignment_mode}\")\n",
    "if saq_gt_dupes > 0 or saq_pred_dupes > 0:\n",
    "    print(f\"Duplicate IDs (kept): GT={saq_gt_dupes}, Pred={saq_pred_dupes}\")\n",
    "print()\n",
    "print(f\"Total questions: {saq_total}\")\n",
    "print(f\"Correct:         {saq_correct}\")\n",
    "print(f\"Missing/Invalid: {saq_missing}\")\n",
    "print(f\"Accuracy:        {saq_accuracy:.4f} ({saq_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Combined results for both MCQ and SAQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Type  Total  Correct  Missing Accuracy Accuracy %\n",
      " MCQ    419      318        0   0.7589     75.89%\n",
      " SAQ    667      136        0   0.2039     20.39%\n",
      "\n",
      "------------------------------------------------------------\n",
      "Overall: 454/1086 = 0.4180 (41.80%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "summary_data = {\n",
    "    \"Type\": [\"MCQ\", \"SAQ\"],\n",
    "    \"Total\": [mcq_total, saq_total],\n",
    "    \"Correct\": [mcq_correct, saq_correct],\n",
    "    \"Missing\": [mcq_missing, saq_missing],\n",
    "    \"Accuracy\": [f\"{mcq_accuracy:.4f}\", f\"{saq_accuracy:.4f}\"],\n",
    "    \"Accuracy %\": [f\"{mcq_accuracy*100:.2f}%\", f\"{saq_accuracy*100:.2f}%\"]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Overall stats\n",
    "total_questions = mcq_total + saq_total\n",
    "total_correct = mcq_correct + saq_correct\n",
    "overall_accuracy = total_correct / total_questions if total_questions > 0 else 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(summary_df.to_string(index=False))\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "print(f\"Overall: {total_correct}/{total_questions} = {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c382c-29d2-45e7-8a0a-ccf820669048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa-model",
   "language": "python",
   "name": "qa-model-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
