{
 "cells": [
  {
   "cell_type": "code",
   "id": "d798dbcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T17:31:58.415572024Z",
     "start_time": "2026-01-16T17:31:58.414765473Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12258412",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.abspath(\"/home/h5/albu670g/qa-model/results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6481a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_paths = {\n",
    "    \"llama_3_8b_instruct\": os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"llama_3_8b_instruct\",\n",
    "        \"codex_high_validated_mcq_llama_3_8b_instruct.tsv\",\n",
    "    ),\n",
    "    \"mistral_7b_instruct\": os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"mistral_7b_instruct\",\n",
    "        \"codex_high_validated_mcq_mistral_7b_instruct.tsv\",\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc1e0bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama_3_8b_instruct</td>\n",
       "      <td>0.622912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral_7b_instruct</td>\n",
       "      <td>0.582339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy\n",
       "0  llama_3_8b_instruct  0.622912\n",
       "1  mistral_7b_instruct  0.582339"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_from_validated(path: str) -> float:\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    return (df[\"conclusion\"]).mean()\n",
    "\n",
    "rows = []\n",
    "for name, path in validated_paths.items():\n",
    "    acc = accuracy_from_validated(path)\n",
    "    rows.append({\"model\": name, \"accuracy\": acc})\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values(\"accuracy\", ascending=False)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e788290e-1b17-4cf7-89f8-caca959bbe26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T17:31:54.019681528Z",
     "start_time": "2026-01-16T17:31:54.018267856Z"
    }
   },
   "source": [
    "import glob\n",
    "\n",
    "CODEX_GROUND_TRUTH = os.path.join(RESULTS_DIR, \"codex\", \"mcq_codex_submission.tsv\")\n",
    "\n",
    "def _pick_choice(row):\n",
    "    true_cols = [c for c in [\"A\", \"B\", \"C\", \"D\"] if str(row[c]).strip().lower() == \"true\"]\n",
    "    if len(true_cols) != 1:\n",
    "        return None\n",
    "    return true_cols[0]\n",
    "\n",
    "def _load_predictions(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df[\"pred_choice\"] = df.apply(_pick_choice, axis=1)\n",
    "    return df[[\"MCQID\", \"pred_choice\"]]\n",
    "\n",
    "codex = _load_predictions(CODEX_GROUND_TRUTH).rename(columns={\"pred_choice\": \"codex_choice\"})\n",
    "\n",
    "model_files = glob.glob(os.path.join(RESULTS_DIR, \"*\", \"mcq_submission*.tsv\"))\n",
    "\n",
    "rows = []\n",
    "for path in sorted(model_files):\n",
    "    model_name = os.path.basename(os.path.dirname(path))\n",
    "    preds = _load_predictions(path)\n",
    "    merged = codex.merge(preds, on=\"MCQID\", how=\"left\")\n",
    "    acc = (merged[\"pred_choice\"] == merged[\"codex_choice\"]).mean()\n",
    "    missing = merged[\"pred_choice\"].isna().sum()\n",
    "    rows.append({\"model\": model_name, \"file\": path, \"accuracy_vs_codex\": acc, \"missing\": missing})\n",
    "\n",
    "codex_results = pd.DataFrame(rows).sort_values(\"accuracy_vs_codex\", ascending=False)\n",
    "codex_results\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3331322913419303"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa-model",
   "language": "python",
   "name": "qa-model-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
