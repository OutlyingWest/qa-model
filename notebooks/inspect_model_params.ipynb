{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Parameters Inspector\n",
    "\n",
    "This notebook allows you to inspect the configuration and parameters of the models used in the project without loading the full weights into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": "%env HF_TOKEN=your_token_here"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, GenerationConfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model cache directory (same as in base_3models.ipynb)\n",
    "CACHE_DIR = os.path.abspath(\"/data/cat/ws/albu670g-qa-model/models\")\n",
    "\n",
    "# List of models to inspect\n",
    "MODEL_IDS = [\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-config-header",
   "metadata": {},
   "source": [
    "## 1. Model Architecture Configuration\n",
    "\n",
    "Load only the config (no weights) to inspect model architecture parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-configs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_config(model_id: str) -> dict:\n",
    "    \"\"\"Load model configuration without loading weights.\"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    return config\n",
    "\n",
    "# Store configs for all models\n",
    "model_configs = {}\n",
    "for model_id in MODEL_IDS:\n",
    "    print(f\"Loading config for: {model_id}\")\n",
    "    try:\n",
    "        model_configs[model_id] = load_model_config(model_id)\n",
    "        print(f\"  ✓ Loaded successfully\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {e}\\n\")\n",
    "        model_configs[model_id] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-configs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full configuration for each model\n",
    "for model_id, config in model_configs.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"MODEL: {model_id}\")\n",
    "    print(\"=\" * 80)\n",
    "    if config is not None:\n",
    "        pprint(config.to_dict())\n",
    "    else:\n",
    "        print(\"Config not available\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 2. Side-by-Side Comparison of Key Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters to compare across models\n",
    "KEY_PARAMS = [\n",
    "    \"architectures\",\n",
    "    \"hidden_size\",\n",
    "    \"intermediate_size\",\n",
    "    \"num_hidden_layers\",\n",
    "    \"num_attention_heads\",\n",
    "    \"num_key_value_heads\",\n",
    "    \"vocab_size\",\n",
    "    \"max_position_embeddings\",\n",
    "    \"rope_theta\",\n",
    "    \"rms_norm_eps\",\n",
    "    \"hidden_act\",\n",
    "    \"tie_word_embeddings\",\n",
    "    \"torch_dtype\",\n",
    "]\n",
    "\n",
    "def extract_key_params(config, params: list) -> dict:\n",
    "    \"\"\"Extract specified parameters from config.\"\"\"\n",
    "    if config is None:\n",
    "        return {p: None for p in params}\n",
    "    config_dict = config.to_dict()\n",
    "    return {p: config_dict.get(p, \"N/A\") for p in params}\n",
    "\n",
    "# Build comparison dataframe\n",
    "comparison_data = {}\n",
    "for model_id, config in model_configs.items():\n",
    "    # Use short name for column header\n",
    "    short_name = model_id.split(\"/\")[-1]\n",
    "    comparison_data[short_name] = extract_key_params(config, KEY_PARAMS)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.index.name = \"Parameter\"\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tokenizer(model_id: str) -> dict:\n",
    "    \"\"\"Load and inspect tokenizer configuration.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    info = {\n",
    "        \"vocab_size\": tokenizer.vocab_size,\n",
    "        \"model_max_length\": tokenizer.model_max_length,\n",
    "        \"is_fast\": tokenizer.is_fast,\n",
    "        \"padding_side\": tokenizer.padding_side,\n",
    "        \"truncation_side\": tokenizer.truncation_side,\n",
    "        \"bos_token\": repr(tokenizer.bos_token),\n",
    "        \"eos_token\": repr(tokenizer.eos_token),\n",
    "        \"pad_token\": repr(tokenizer.pad_token),\n",
    "        \"unk_token\": repr(tokenizer.unk_token),\n",
    "        \"has_chat_template\": hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None,\n",
    "    }\n",
    "    return info, tokenizer\n",
    "\n",
    "# Inspect tokenizers\n",
    "tokenizer_infos = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for model_id in MODEL_IDS:\n",
    "    print(f\"Loading tokenizer for: {model_id}\")\n",
    "    try:\n",
    "        info, tok = inspect_tokenizer(model_id)\n",
    "        tokenizer_infos[model_id] = info\n",
    "        tokenizers[model_id] = tok\n",
    "        print(f\"  ✓ Loaded successfully\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {e}\\n\")\n",
    "        tokenizer_infos[model_id] = None\n",
    "        tokenizers[model_id] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer comparison dataframe\n",
    "tok_comparison = {}\n",
    "for model_id, info in tokenizer_infos.items():\n",
    "    short_name = model_id.split(\"/\")[-1]\n",
    "    tok_comparison[short_name] = info if info else {}\n",
    "\n",
    "tok_df = pd.DataFrame(tok_comparison)\n",
    "tok_df.index.name = \"Property\"\n",
    "tok_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-template-header",
   "metadata": {},
   "source": [
    "## 4. Chat Templates\n",
    "\n",
    "View the chat template used by each model for formatting conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-templates",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id, tok in tokenizers.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"CHAT TEMPLATE: {model_id}\")\n",
    "    print(\"=\" * 80)\n",
    "    if tok is not None and hasattr(tok, \"chat_template\") and tok.chat_template:\n",
    "        print(tok.chat_template)\n",
    "    else:\n",
    "        print(\"No chat template defined\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation-header",
   "metadata": {},
   "source": [
    "## 5. Generation Configuration\n",
    "\n",
    "Default generation parameters for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in MODEL_IDS:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"GENERATION CONFIG: {model_id}\")\n",
    "    print(\"=\" * 80)\n",
    "    try:\n",
    "        gen_config = GenerationConfig.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "        pprint(gen_config.to_dict())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load generation config: {e}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-tokens-header",
   "metadata": {},
   "source": [
    "## 6. Special Tokens Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id, tok in tokenizers.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SPECIAL TOKENS: {model_id}\")\n",
    "    print(\"=\" * 80)\n",
    "    if tok is not None:\n",
    "        print(f\"All special tokens: {tok.all_special_tokens}\")\n",
    "        print(f\"Special tokens map: {tok.special_tokens_map}\")\n",
    "        if hasattr(tok, \"added_tokens_encoder\") and tok.added_tokens_encoder:\n",
    "            print(f\"Added tokens: {list(tok.added_tokens_encoder.keys())}\")\n",
    "    else:\n",
    "        print(\"Tokenizer not available\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-size-header",
   "metadata": {},
   "source": [
    "## 7. Estimated Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-size",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_params(config) -> int:\n",
    "    \"\"\"Estimate total number of parameters from config.\n",
    "    \n",
    "    This is an approximation based on typical transformer architecture.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        return 0\n",
    "    \n",
    "    cfg = config.to_dict()\n",
    "    \n",
    "    vocab_size = cfg.get(\"vocab_size\", 0)\n",
    "    hidden_size = cfg.get(\"hidden_size\", 0)\n",
    "    num_layers = cfg.get(\"num_hidden_layers\", 0)\n",
    "    intermediate_size = cfg.get(\"intermediate_size\", 0)\n",
    "    num_heads = cfg.get(\"num_attention_heads\", 0)\n",
    "    num_kv_heads = cfg.get(\"num_key_value_heads\", num_heads)\n",
    "    \n",
    "    # Embedding parameters\n",
    "    embed_params = vocab_size * hidden_size\n",
    "    \n",
    "    # Attention parameters per layer (Q, K, V, O projections)\n",
    "    head_dim = hidden_size // num_heads if num_heads > 0 else 0\n",
    "    q_params = hidden_size * hidden_size\n",
    "    k_params = hidden_size * (num_kv_heads * head_dim)\n",
    "    v_params = hidden_size * (num_kv_heads * head_dim)\n",
    "    o_params = hidden_size * hidden_size\n",
    "    attn_params = q_params + k_params + v_params + o_params\n",
    "    \n",
    "    # MLP parameters per layer\n",
    "    mlp_params = 3 * hidden_size * intermediate_size  # gate, up, down projections\n",
    "    \n",
    "    # Layer norm parameters per layer\n",
    "    ln_params = 2 * hidden_size\n",
    "    \n",
    "    # Total per layer\n",
    "    layer_params = attn_params + mlp_params + ln_params\n",
    "    \n",
    "    # Total model\n",
    "    total = embed_params + (num_layers * layer_params) + hidden_size  # final layer norm\n",
    "    \n",
    "    # If not tie_word_embeddings, add output projection\n",
    "    if not cfg.get(\"tie_word_embeddings\", True):\n",
    "        total += vocab_size * hidden_size\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Calculate and display model sizes\n",
    "size_data = []\n",
    "for model_id, config in model_configs.items():\n",
    "    params = estimate_model_params(config)\n",
    "    size_data.append({\n",
    "        \"Model\": model_id.split(\"/\")[-1],\n",
    "        \"Estimated Parameters\": f\"{params:,}\",\n",
    "        \"Size (Billions)\": f\"{params / 1e9:.2f}B\",\n",
    "        \"FP16 Memory (GB)\": f\"{params * 2 / 1e9:.2f}\",\n",
    "        \"FP32 Memory (GB)\": f\"{params * 4 / 1e9:.2f}\",\n",
    "    })\n",
    "\n",
    "pd.DataFrame(size_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}