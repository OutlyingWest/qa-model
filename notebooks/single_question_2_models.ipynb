{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20656945",
   "metadata": {},
   "source": [
    "# Single-Question Inference (2 Base Models)\n",
    "\n",
    "Use this notebook to ask one question to either Mistral-7B-Instruct or Llama-3-8B-Instruct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d11e5",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- A GPU is recommended (7B/8B models are large).\n",
    "- Llama-3 models require accepting the Meta license on Hugging Face.\n",
    "- Models are pre-cached on the cluster at `/data/cat/ws/albu670g-qa-model/models`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c341c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%export` not found.\n"
     ]
    }
   ],
   "source": [
    "# Core deps\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8daefa-bf85-4558-9123-b2381f24455c",
   "metadata": {},
   "outputs": [],
   "source": "os.environ[\"HF_TOKEN\"] = \"\""
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98d1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model options (two base models)\n",
    "MODEL_IDS = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "\n",
    "# Select which model to use\n",
    "selected_key = \"mistral\"  # change to \"llama3\"\n",
    "model_id = MODEL_IDS[selected_key]\n",
    "\n",
    "# Cache directory for models (pre-cached on the cluster)\n",
    "CACHE_DIR = os.path.abspath(\"/data/cat/ws/albu670g-qa-model/models\")\n",
    "if not os.path.isdir(CACHE_DIR):\n",
    "    # Fallback to a local cache if the cluster path is unavailable\n",
    "    CACHE_DIR = os.path.abspath(\"../models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceef8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab27a23622b34478b138c7b567d85e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    dtype=dtype,\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str) -> str:\n",
    "    \"\"\"Build a model-appropriate prompt.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    chat_template = getattr(tokenizer, \"chat_template\", None)\n",
    "    if chat_template:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    return f\"System: {messages[0]['content']}\\nUser: {question}\\nAssistant:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa6a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(question: str, max_new_tokens: int = 128, temperature: float = 0.0) -> str:\n",
    "    \"\"\"Run a single-question generation and return the model answer.\"\"\"\n",
    "    prompt = build_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    do_sample = temperature > 0\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": do_sample,\n",
    "    }\n",
    "    if do_sample:\n",
    "        gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "    pad_token_id = tokenizer.eos_token_id or tokenizer.pad_token_id\n",
    "    if pad_token_id is not None:\n",
    "        gen_kwargs[\"pad_token_id\"] = pad_token_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[-1] :]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35970fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask one question\n",
    "question = \"What is the capital of France?\"\n",
    "answer = ask_model(question)\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa-model",
   "language": "python",
   "name": "qa-model-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
