\section{Implementation}
The implementation builds on PyTorch and the Hugging Face Transformers stack
\citep{Paszke2019PyTorch,Wolf2020Transformers}, with Hydra used for
configuration management \citep{Yadan2019Hydra}. The codebase separates
training and inference scripts for clarity and reproducibility.

\subsection{LoRA Fine-Tuning}
LoRA adapters \citep{Hu2021LoRA} are applied to the attention projection layers
(\texttt{q\_proj}, \texttt{k\_proj}, \texttt{v\_proj}, \texttt{o\_proj}) with
rank $r=16$, $\alpha=32$, and dropout $0.05$. This keeps the number of trainable
parameters small while allowing task-specific adaptation. For SAQ, an extended
configuration that includes \texttt{gate\_proj} is also tested to increase MLP
capacity.

\subsection{MCQ Inference via Logprob Scoring}
Instead of generating a full answer, MCQ inference scores each option by the
log-probability of producing a single-letter continuation. The algorithm
performs a forward pass on the prompt to obtain the KV cache and the
distribution for the next token. For each choice in \{A,B,C,D\}, several
textual variants (`` A'', ``\textbackslash nA'', ``A'') are evaluated and
token-level log-probabilities are summed using the cached states. The best
variant score is selected for that choice, and the highest-scoring choice is
returned.

An optional lightweight country-aware prior is added derived from the MCQ
training data. The prior is computed as a smoothed log-probability of the
target country tag conditioned on the option text, and scaled by a tunable
weight during inference.

\subsection{RAG for SAQ}
For SAQ, a retrieval-augmented generation path is integrated
\citep{Lewis2020RAG}. The retriever builds a Wikipedia-based index using
BM25 \citep{Robertson2009BM25} over tokenized document text. Tokenization
lowercases, strips punctuation, removes stop words, and applies a lightweight
Porter stemmer \citep{Porter1980}. At inference time, the top-$k$ passages are
inserted into a dedicated RAG prompt template. Contexts can also be
precomputed and loaded from disk to avoid on-the-fly retrieval.

\subsection{Parsing and Validation}
The system enforces strict answer formats to stabilize evaluation. SAQ answers
must follow a single-line ``Answer: <ANSWER>'' template with 1--6 tokens, while
MCQ responses must include exactly one of \{A,B,C,D\}. For SAQ, additional
regex checks enforce dataset-specific formats (e.g., \texttt{HH:MM} or bounded
integer ranges). When validation fails, the model is prompted to retry up to a
small fixed number of times, and the final response is parsed deterministically.
