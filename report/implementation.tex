\section{Implementation}
The implementation builds on PyTorch and the Hugging Face Transformers stack
\citep{Paszke2019PyTorch,Wolf2020Transformers}, with Hydra used for
configuration management \citep{Yadan2019Hydra}. The codebase separates
training and inference scripts for clarity and reproducibility.

\paragraph{Model choice.}
All reported experiments use \textbf{LLaMA-3B-Instruct}. Early trials with the
base (non-instruct) LLaMA-3B produced very low accuracy and did not benefit
meaningfully from LoRA; because the goal was to study parameter-efficient
fine-tuning in a realistic setting, we switched to the instruct variant to
start from a model aligned to task directives. Config files allow other models,
but the results in this report are from the 3B Instruct backbone.

\subsection{LoRA Fine-Tuning}
LoRA adapters \citep{Hu2021LoRA} are applied to the attention projection layers
(\texttt{q\_proj}, \texttt{k\_proj}, \texttt{v\_proj}, \texttt{o\_proj}) with
rank $r=16$, $\alpha=32$, and dropout $0.05$. This keeps the number of trainable
parameters small while allowing task-specific adaptation. For SAQ, an extended
configuration that includes \texttt{gate\_proj} is also tested to increase MLP
capacity.

\subsection{MCQ Inference via Logprob Scoring}
Instead of generating a full answer, MCQ inference scores each option by the
log-probability of producing a single-letter continuation. The algorithm
performs a forward pass on the prompt to obtain the KV cache and the
distribution for the next token. For each choice in \{A,B,C,D\}, several
textual variants (`` A'', ``\textbackslash nA'', ``A'') are evaluated and
token-level log-probabilities are summed using the cached states. The best
variant score is selected for that choice, and the highest-scoring choice is
returned.

An optional country-aware reranking bonus is computed from
\texttt{train\_dataset\_mcq.csv} with Laplace smoothing $\alpha{=}1$ over
normalized option texts and country tags. The final score for choice $c$ is:
\begin{equation}
\begin{aligned}
s(c) &= \log P(c \mid \text{prompt}) \\
&\quad + w\bigl(\log P_{\text{prior}}(c \mid \text{country})
 - \log P_{\text{uniform}}\bigr),
\end{aligned}
\end{equation}
where $w$ is the rerank weight and the uniform term centers scores across
countries. A sweep over $w\in\{1.0,1.4,2.0,2.5\}$ favored $w{=}2.5$, which is
used in the shipped inference config to slightly strengthen country-consistent
ties while keeping overall accuracy stable.

\subsection{RAG for SAQ}
For SAQ, a retrieval-augmented generation path is integrated
\citep{Lewis2020RAG}. The retriever builds a Wikipedia-based index using
BM25 \citep{Robertson2009BM25} with $k_1{=}1.5$, $b{=}0.75$ over tokenized
document text. Tokenization lowercases, removes punctuation, filters English
stop words, and applies a lightweight Porter stemmer \citep{Porter1980}. At
inference time the top-$k{=}3$ passages (up to 512 tokens of context) are
inserted into a dedicated RAG prompt template. Contexts can also be
precomputed and loaded from disk to avoid on-the-fly retrieval. Corpus and
index live under \texttt{rag\_dir} (configurable); corpus size statistics were
not logged, which limits visibility into coverage and retrieval quality.

\subsection{Parsing and Validation}
The system enforces strict answer formats to stabilize evaluation. SAQ answers
must follow a single-line ``Answer: <ANSWER>'' template with 1--6 tokens, while
MCQ responses must include exactly one of \{A,B,C,D\}. For SAQ, additional
regex checks enforce dataset-specific formats (e.g., \texttt{HH:MM} or bounded
integer ranges). When validation fails, the model is prompted to retry up to a
small fixed number of times, and the final response is parsed deterministically.
