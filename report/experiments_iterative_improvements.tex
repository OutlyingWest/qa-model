\section{Experiments and Iterative Improvements}
\label{sec:experiments}
This section summarizes the iterative changes evaluated during development.
The ordering follows the experiment log, and each entry corresponds to a
submission and related code changes. Quantitative results are reported in
Section~\ref{sec:results}; this section focuses on the intent and design of
each modification.

\subsection{Experimental Setup}
Separate LoRA adapters were trained for MCQ and SAQ using the project Hydra
configuration. Unless otherwise stated, LoRA uses $r=16$, $\alpha=32$, dropout
0.05, and targets attention projections only. MCQ training uses 3 epochs, batch
size 4, gradient accumulation 4, learning rate $1\times10^{-4}$, and a cosine
scheduler. SAQ training uses 3 epochs, batch size 4, gradient accumulation 4,
learning rate $1\times10^{-5}$, and a constant scheduler. Inference is
deterministic (temperature 0) with a maximum of 16 generated tokens and up to
two validation retries.

Metrics are accuracy overall and by country (CN, IR, GB, US) for both tasks.
All runs were executed on a single NVIDIA H100 GPU (95,830 MiB), driver
580.65.06, CUDA 13.0.

\subsection{Iterative Improvement Summary}
Table~\ref{tab:submissions} summarizes the primary changes and their
approximate effects on accuracy. This summary anchors the narrative to the
experiment log without listing submission identifiers.

\begin{table}[t]
  \centering
  \small
  \setlength{\tabcolsep}{4pt}
  \begin{tabularx}{\columnwidth}{Xcc}
    \hline
    Change & Task & $\Delta$Accuracy (pp) \\
    \hline
    Prompt+parsing refinement & SAQ & +8 \\
    Validation retries & SAQ & $\sim$0 \\
    Add \texttt{gate\_proj} LoRA targets & SAQ & +1 \\
    Logprob scoring with $w=1.0$ & MCQ & +3 \\
    Logprob scoring with $w=1.4$ & MCQ & +4 \\
    Logprob scoring with $w=2.0$ & MCQ & +5 \\
    Logprob scoring ($w=2.5$) & MCQ & +5 (ties) \\
    \hline
  \end{tabularx}
  \caption{Summary of iterative changes with approximate accuracy deltas
  (percentage points) relative to the baseline.}
  \label{tab:submissions}
\end{table}

\subsection{Baseline}
The baseline uses LoRA adapters on attention projections, standard task
prompts, and format validation with limited retries. MCQ uses log-probability
scoring, while SAQ uses constrained generation with a strict ``Answer:''
prefix. Baseline accuracy is 0.74 for MCQ and 0.50 for SAQ (Table~\ref{tab:mcq}
and Table~\ref{tab:saq}).

\subsection{SAQ Iterations}
Three categories of changes were evaluated for SAQ: prompt/format refinements,
validation retries, and expanded LoRA target layers. These are designed to
reduce parsing errors, support multiword answers, and increase adapter
capacity without full fine-tuning.

\paragraph{Prompt and parsing refinement.}
The SAQ prompt was updated to enforce a strict single-line ``Answer:'' format
and parsing was extended to accept multiword answers. This yielded an
improvement of approximately 8 percentage points in SAQ accuracy, primarily
by reducing formatting errors.

\paragraph{Validation retries.}
A lightweight retry mechanism was added for invalid SAQ outputs. In practice
this affected only four answers in the evaluation set and did not change
aggregate accuracy in a meaningful way.

\paragraph{Extended LoRA targets.}
The SAQ adapter was retrained with \texttt{gate\_proj} enabled, increasing MLP
capacity. This provided a modest additional gain of about 1 percentage point.

\subsection{MCQ Iterations}
For MCQ, logprob variants and reranking weights were tested. The goal is to
stabilize single-letter selection and leverage country priors derived from the
training set when options are ambiguous. A sweep over $w\in\{1.0,1.4,2.0,2.5\}$
showed $w{=}2.0$ and $w{=}2.5$ delivering the largest gains; the final config
fixes $w{=}2.5$ because it slightly improves tie-break consistency without
changing macro accuracy relative to $w{=}2.0$ (Table~\ref{tab:mcq}).

\subsection{RAG Variants}
RAG experiments compare raw retrieval, stop-word filtering, and stemming
configurations. A later RAG training run and the resulting inference behavior
were analyzed to assess whether retrieval helps SAQ accuracy.

Raw retrieval on SAQ reduces accuracy and noticeably increases ``idk'' outputs.
The index was rebuilt with stop-word removal and stemming; the final RAG
training run shows a steady loss decrease to about 1.55 by 1.5 epochs and a
plateau around 1.46--1.52 by 3 epochs. Validation improves from
eval\_loss 1.545 to 1.466 and eval\_mean\_token\_accuracy from 0.671 to 0.680,
with entropy decreasing from roughly 1.46 to 1.42. The trend is positive but
the accuracy gains saturate after about two epochs, suggesting the need to
audit generation quality and data coverage.

At inference time, no meaningful SAQ improvement was observed from the current
RAG implementation. The last RAG run achieves 0.58 SAQ accuracy overall with
country scores 0.51 (CN), 0.63 (GB), 0.50 (IR), and 0.70 (US), which is
slightly below the best non-RAG configuration (Table~\ref{tab:saq}). MCQ
accuracy remains unchanged at 0.79 because RAG is applied only to SAQ.

Potential improvements include tuning $k$ and context length, filtering noisy
retrievals with a confidence threshold, and experimenting with learned
retrievers or reranking to reduce irrelevant context.
