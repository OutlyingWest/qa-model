\section{Conclusion and Future Work}
This work presents a LoRA-based QA pipeline for MCQ and SAQ with strict format
control, log-probability scoring for MCQ, and an optional BM25-based RAG
component for SAQ. Experiments are run on an LLaMA-3B-Instruct backbone; early
attempts with the base (non-instruct) model underperformed badly and did not
benefit from LoRA, motivating the switch to the instruct variant. Across the
iteration cycle, the largest SAQ gains come from
prompt and parsing refinements, with a smaller contribution from expanding LoRA
targets to include \texttt{gate\_proj}. For MCQ, logprob scoring with tuned
weighting improves accuracy over the baseline. The best combined configuration
reaches 0.79 MCQ accuracy and 0.59 SAQ accuracy using $w{=}2.5$, while the
current RAG implementation does not yield a measurable SAQ improvement.

Future work should focus on (1) improving retrieval quality with better
indexing, reranking, or learned retrievers; (2) analyzing generation errors to
separate formatting failures from semantic mistakes; (3) expanding ablations on
LoRA targets and hyperparameters; and (4) adding robust reporting of hardware
and training-time metrics to improve reproducibility.
